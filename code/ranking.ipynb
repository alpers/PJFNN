{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform the ranking tasks\n",
    "\n",
    "- randomly select 20 candidate jobs and rank them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, recall_score, precision_score, f1_score, classification_report\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_result(y_true, y_prob):\n",
    "    y_prediction = [0 if i<=0.5 else 1 for i in y_prob]\n",
    "    report = classification_report(y_true,y_prediction,digits=4)\n",
    "    report = report.splitlines()\n",
    "    columns = ['class'] + report[0].split()\n",
    "    col_1, col_2, col_3, col_4, col_5 = [], [], [], [], []\n",
    "    for row in report[1:]:\n",
    "        if len(row.split()) != 0:\n",
    "            row = row.split()\n",
    "            if len(row) < 5:\n",
    "                col_1.append(row[0])\n",
    "                col_2.append('')\n",
    "                col_3.append('')\n",
    "                col_4.append(row[1])\n",
    "                col_5.append(row[2])\n",
    "            elif len(row) > 5:\n",
    "                col_1.append(row[0] + ' ' + row[1])\n",
    "                col_2.append(row[2])\n",
    "                col_3.append(row[3])\n",
    "                col_4.append(row[4])\n",
    "                col_5.append(row[5])\n",
    "            else:\n",
    "                col_1.append(row[0])\n",
    "                col_2.append(row[1])\n",
    "                col_3.append(row[2])\n",
    "                col_4.append(row[3])\n",
    "                col_5.append(row[4])\n",
    "    col_1.append(\"overall\")\n",
    "    col_2.append(precision_score(y_true, y_prediction))\n",
    "    col_3.append(recall_score(y_true, y_prediction))\n",
    "    col_4.append(f1_score(y_true, y_prediction))\n",
    "    col_5.append(roc_auc_score(y_true, y_prob))\n",
    "    result = pd.DataFrame()\n",
    "    result[columns[0]] = col_1\n",
    "    result[columns[1]] = col_2\n",
    "    result[columns[2]] = col_3\n",
    "    result[columns[3]] = col_4\n",
    "    result[columns[4]] = col_5\n",
    "    print(\"——————Test——————\")\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_set = pd.read_csv(\"user_set_cleaned.csv\")\n",
    "job_set = pd.read_csv(\"job_set_cleaned.csv\")\n",
    "work_history = pd.read_csv(\"work_history_cleaned.csv\")\n",
    "dataset = pd.read_csv(\"dataset_cleaned.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.load(\"X_train.npy\")\n",
    "Y_train = np.load(\"Y_train.npy\")\n",
    "X_test = np.load(\"X_test.npy\")\n",
    "Y_test = np.load(\"Y_test.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Build datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# about 1 min\n",
    "job_set = job_set.fillna(\" \")\n",
    "job_set[\"word\"] = job_set.Title + job_set.Description + job_set.Requirements\n",
    "tf = TfidfVectorizer(analyzer='word',ngram_range=(1, 2),min_df=5, max_features=100, stop_words='english')\n",
    "tfidf_matrix = tf.fit_transform(job_set['word'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_history_tf = TfidfVectorizer(analyzer='word',ngram_range=(1, 2),min_df=0, max_features=50, stop_words='english')\n",
    "word_history_tf_matrix = word_history_tf.fit_transform(work_history.groupby(\"UserID\").JobTitle.sum().values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_user = user_set[user_set.Split==\"Test\"].UserID.values\n",
    "test_data = dataset[dataset.UserID.isin(test_user)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 260/260 [00:04<00:00, 57.42it/s]\n"
     ]
    }
   ],
   "source": [
    "ranking_data = pd.DataFrame(columns = [\"UserID\",\"JobID\",\"label\", \"City\", \"State\"])\n",
    "job_id = job_set.JobID.unique().tolist()\n",
    "groups = test_data.groupby(\"UserID\")\n",
    "user_ids = []\n",
    "job_ids = []\n",
    "labels = []\n",
    "City = []\n",
    "State = []\n",
    "for idx, group in tqdm(groups):\n",
    "    size = 99\n",
    "    exist_job = group.JobID.unique().tolist()\n",
    "    candidate_job = [i for i in job_id if i not in exist_job ]\n",
    "    sample_job = random.sample(range(0,len(candidate_job)),size)\n",
    "    user_ids.extend([idx] * (size+1))\n",
    "    job_ids.append(exist_job[0])\n",
    "    job_ids.extend([candidate_job[i] for i in sample_job])\n",
    "    labels.append(1)\n",
    "    labels.extend([0] * (size))\n",
    "    City.append(group.City.values[0])\n",
    "    State.append(group.State.values[0])\n",
    "    jobs = job_set[job_set.JobID.isin([candidate_job[i] for i in sample_job])]\n",
    "    \n",
    "    City.extend([0 if i!=group.City.values[0] else a for i in jobs.City.values.tolist()])\n",
    "    State.extend([0 if i!=group.State.values[0] else a for i in jobs.State.values.tolist()])\n",
    "    \n",
    "ranking_data.UserID = user_ids\n",
    "ranking_data.JobID = job_ids\n",
    "ranking_data.label = labels\n",
    "ranking_data.City = City\n",
    "ranking_data.State = State\n",
    "# ranking_data.to_csv(\"ranking_data.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Define the evaluation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_hit_rate(model, N):\n",
    "    hit = 0\n",
    "    groups = ranking_data.groupby(\"UserID\")\n",
    "    for u_id, group in tqdm(groups):\n",
    "        X = np.zeros((1,158))\n",
    "        user = user_set[user_set.UserID==u_id][[\"DegreeType\", \"WorkHistoryCount\", \"TotalYearsExperience\", \"CurrentlyEmployed\", \n",
    "                                                \"ManagedOthers\", \"ManagedHowMany\"]]\n",
    "        u_idx = user.index.values[0]\n",
    "        user_feature = np.concatenate((user.values, word_history_tf_matrix[u_idx,:].toarray()),axis=1)\n",
    "        job_id_list = group.JobID.values\n",
    "        jobs = job_set[job_set.JobID.isin(job_id_list)]\n",
    "        j_idx = jobs.index.values\n",
    "        f = []\n",
    "        for i in j_idx:\n",
    "            feature = np.concatenate((user_feature, tfidf_matrix[i,:].toarray()), axis=1).reshape(156,).tolist()\n",
    "            f.append(feature)\n",
    "        feature = np.concatenate((group[[\"City\",\"State\"]].values, np.array(f)),axis=1)\n",
    "        X = np.concatenate((X, feature), axis=0)\n",
    "        result = model.predict_proba(X[1:])\n",
    "#         result = model.predict(X[1:])\n",
    "        a = -np.sort(-result[:,1])\n",
    "        idx = np.argwhere(a==result[0,1])[0][0]\n",
    "        if idx <= N-1:\n",
    "            hit += 1\n",
    "    return hit/len(test_user)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Test models\n",
    "- Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "——————Test——————\n",
      "          class precision    recall  f1-score   support\n",
      "0             0    0.6395    0.6262    0.6328       527\n",
      "1             1    0.6338    0.6471    0.6404       527\n",
      "2      accuracy                        0.6366      1054\n",
      "3     macro avg    0.6367    0.6366    0.6366      1054\n",
      "4  weighted avg    0.6367    0.6366    0.6366      1054\n",
      "5       overall  0.633829  0.647059  0.640376  0.702132\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestClassifier(random_state=0)\n",
    "rf.fit(X_train,Y_train)\n",
    "y_pred = rf.predict_proba(X_test)\n",
    "show_result(Y_test, y_pred[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 260/260 [00:07<00:00, 32.61it/s]\n",
      "100%|██████████| 260/260 [00:07<00:00, 32.54it/s]\n",
      "100%|██████████| 260/260 [00:07<00:00, 32.87it/s]\n",
      "100%|██████████| 260/260 [00:07<00:00, 33.57it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.026923076923076925,\n",
       " 0.12307692307692308,\n",
       " 0.22692307692307692,\n",
       " 0.4307692307692308)"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_hit_rate(rf,1), test_hit_rate(rf,5), test_hit_rate(rf,10), test_hit_rate(rf,20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "——————Test——————\n",
      "          class precision    recall  f1-score   support\n",
      "0             0    0.5418    0.5655    0.5534       527\n",
      "1             1    0.5456    0.5218    0.5335       527\n",
      "2      accuracy                        0.5436      1054\n",
      "3     macro avg    0.5437    0.5436    0.5434      1054\n",
      "4  weighted avg    0.5437    0.5436    0.5434      1054\n",
      "5       overall  0.545635  0.521822  0.533463  0.548738\n"
     ]
    }
   ],
   "source": [
    "def test_hit_rate_linearRegr(model, N):\n",
    "    hit = 0\n",
    "    groups = ranking_data.groupby(\"UserID\")\n",
    "    for u_id, group in tqdm(groups):\n",
    "        X = np.zeros((1,158))\n",
    "        user = user_set[user_set.UserID==u_id][[\"DegreeType\", \"WorkHistoryCount\", \"TotalYearsExperience\", \"CurrentlyEmployed\", \n",
    "                                                \"ManagedOthers\", \"ManagedHowMany\"]]\n",
    "        u_idx = user.index.values[0]\n",
    "        user_feature = np.concatenate((user.values, word_history_tf_matrix[u_idx,:].toarray()),axis=1)\n",
    "        job_id_list = group.JobID.values\n",
    "        jobs = job_set[job_set.JobID.isin(job_id_list)]\n",
    "        j_idx = jobs.index.values\n",
    "        f = []\n",
    "        for i in j_idx:\n",
    "            feature = np.concatenate((user_feature, tfidf_matrix[i,:].toarray()), axis=1).reshape(156,).tolist()\n",
    "            f.append(feature)\n",
    "        feature = np.concatenate((group[[\"City\",\"State\"]].values, np.array(f)),axis=1)\n",
    "        X = np.concatenate((X, feature), axis=0)\n",
    "#         result = model.predict_proba(X[1:])\n",
    "        result = model.predict(X[1:])\n",
    "        a = -np.sort(-result)\n",
    "        idx = np.argwhere(a==result[0])[0][0]\n",
    "        if idx <= N-1:\n",
    "            hit += 1\n",
    "    return hit/len(test_user)\n",
    "linear_r = LinearRegression()\n",
    "linear_r.fit(X_train,Y_train)\n",
    "y_pred = linear_r.predict(X_test)\n",
    "show_result(Y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 260/260 [00:04<00:00, 64.74it/s]\n",
      "100%|██████████| 260/260 [00:03<00:00, 65.60it/s]\n",
      "100%|██████████| 260/260 [00:03<00:00, 68.62it/s]\n",
      "100%|██████████| 260/260 [00:03<00:00, 67.53it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.007692307692307693, 0.1, 0.1576923076923077, 0.28846153846153844)"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_hit_rate_linearRegr(linear_r,1), test_hit_rate_linearRegr(linear_r,5), test_hit_rate_linearRegr(linear_r,10), test_hit_rate_linearRegr(linear_r, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "——————Test——————\n",
      "          class precision    recall  f1-score  support\n",
      "0             0    0.5326    0.5579    0.5449      527\n",
      "1             1    0.5359    0.5104    0.5228      527\n",
      "2      accuracy                        0.5342     1054\n",
      "3     macro avg    0.5342    0.5342    0.5339     1054\n",
      "4  weighted avg    0.5342    0.5342    0.5339     1054\n",
      "5       overall  0.535857  0.510436  0.522838  0.55033\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression()\n",
    "lr.fit(X_train,Y_train)\n",
    "y_pred = lr.predict_proba(X_test)\n",
    "show_result(Y_test, y_pred[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 260/260 [00:03<00:00, 66.40it/s]\n",
      "100%|██████████| 260/260 [00:03<00:00, 65.50it/s]\n",
      "100%|██████████| 260/260 [00:03<00:00, 66.03it/s]\n",
      "100%|██████████| 260/260 [00:04<00:00, 61.75it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.007692307692307693, 0.09230769230769231, 0.1423076923076923, 0.3)"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_hit_rate(lr,1), test_hit_rate(lr,5), test_hit_rate(lr,10), test_hit_rate(lr,20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "——————Test——————\n",
      "          class precision    recall  f1-score   support\n",
      "0             0    0.6022    0.6205    0.6112       527\n",
      "1             1    0.6086    0.5901    0.5992       527\n",
      "2      accuracy                        0.6053      1054\n",
      "3     macro avg    0.6054    0.6053    0.6052      1054\n",
      "4  weighted avg    0.6054    0.6053    0.6052      1054\n",
      "5       overall  0.608611  0.590133  0.599229  0.630908\n"
     ]
    }
   ],
   "source": [
    "dt = DecisionTreeClassifier(max_leaf_nodes=1500,random_state=0)\n",
    "dt.fit(X_train,Y_train)\n",
    "y_pred = dt.predict_proba(X_test)\n",
    "show_result(Y_test, y_pred[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 260/260 [00:04<00:00, 59.88it/s]\n",
      "100%|██████████| 260/260 [00:03<00:00, 66.21it/s]\n",
      "100%|██████████| 260/260 [00:03<00:00, 68.08it/s]\n",
      "100%|██████████| 260/260 [00:04<00:00, 64.94it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.03076923076923077,\n",
       " 0.08076923076923077,\n",
       " 0.16153846153846155,\n",
       " 0.3038461538461538)"
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_hit_rate(dt,1), test_hit_rate(dt,5), test_hit_rate(dt,10), test_hit_rate(dt,20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "——————Test——————\n",
      "          class precision   recall  f1-score   support\n",
      "0             0    0.5125   0.5825    0.5453       527\n",
      "1             1    0.5165   0.4459    0.4786       527\n",
      "2      accuracy                       0.5142      1054\n",
      "3     macro avg    0.5145   0.5142    0.5120      1054\n",
      "4  weighted avg    0.5145   0.5142    0.5120      1054\n",
      "5       overall  0.516484  0.44592  0.478615  0.530636\n"
     ]
    }
   ],
   "source": [
    "nb = GaussianNB()\n",
    "nb.fit(X_train,Y_train)\n",
    "y_pred = nb.predict_proba(X_test)\n",
    "show_result(Y_test, y_pred[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 260/260 [00:03<00:00, 66.77it/s]\n",
      "100%|██████████| 260/260 [00:05<00:00, 45.99it/s]\n",
      "100%|██████████| 260/260 [00:04<00:00, 62.23it/s]\n",
      "100%|██████████| 260/260 [00:03<00:00, 67.78it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.015384615384615385,\n",
       " 0.08461538461538462,\n",
       " 0.2076923076923077,\n",
       " 0.36923076923076925)"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_hit_rate(nb,1), test_hit_rate(nb,5), test_hit_rate(nb,10), test_hit_rate(nb,20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "——————Test——————\n",
      "          class precision    recall  f1-score   support\n",
      "0             0    0.5228    0.5863    0.5528       527\n",
      "1             1    0.5292    0.4649    0.4949       527\n",
      "2      accuracy                        0.5256      1054\n",
      "3     macro avg    0.5260    0.5256    0.5239      1054\n",
      "4  weighted avg    0.5260    0.5256    0.5239      1054\n",
      "5       overall  0.529158  0.464896  0.494949  0.534969\n"
     ]
    }
   ],
   "source": [
    "ada = AdaBoostClassifier(random_state=0)\n",
    "ada.fit(X_train,Y_train)\n",
    "y_pred = ada.predict_proba(X_test)\n",
    "show_result(Y_test, y_pred[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 260/260 [00:05<00:00, 44.07it/s]\n",
      "100%|██████████| 260/260 [00:06<00:00, 38.27it/s]\n",
      "100%|██████████| 260/260 [00:05<00:00, 43.52it/s]\n",
      "100%|██████████| 260/260 [00:06<00:00, 40.42it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.019230769230769232,\n",
       " 0.12307692307692308,\n",
       " 0.21153846153846154,\n",
       " 0.3269230769230769)"
      ]
     },
     "execution_count": 293,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_hit_rate(ada,1), test_hit_rate(ada,5), test_hit_rate(ada,10), test_hit_rate(ada,20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Iter       Train Loss   Remaining Time \n",
      "         1           1.3806            4.76m\n",
      "         2           1.3771            5.20m\n",
      "         3           1.3720            5.37m\n",
      "         4           1.3695            5.30m\n",
      "         5           1.3652            5.15m\n",
      "         6           1.3574            5.05m\n",
      "         7           1.3522            4.97m\n",
      "         8           1.3505            4.98m\n",
      "         9           1.3450            4.93m\n",
      "        10           1.3371            4.81m\n",
      "        20           1.3059            4.25m\n",
      "        30           1.2817            3.70m\n",
      "        40           1.2592            3.21m\n",
      "        50           1.2361            2.69m\n",
      "        60           1.2155            2.15m\n",
      "        70           1.1986            1.63m\n",
      "        80           1.1873            1.09m\n",
      "        90           1.1682           32.63s\n",
      "       100           1.1548            0.00s\n",
      "——————Test——————\n",
      "          class precision    recall  f1-score   support\n",
      "0             0    0.6278    0.6338    0.6308       527\n",
      "1             1    0.6303    0.6243    0.6273       527\n",
      "2      accuracy                        0.6290      1054\n",
      "3     macro avg    0.6290    0.6290    0.6290      1054\n",
      "4  weighted avg    0.6290    0.6290    0.6290      1054\n",
      "5       overall  0.630268  0.624288  0.627264  0.629032\n"
     ]
    }
   ],
   "source": [
    "gbdt = GradientBoostingClassifier(max_depth=10, random_state=0, verbose=1)\n",
    "gbdt.fit(X_train,Y_train)\n",
    "y_pred = gbdt.predict_proba(X_test)\n",
    "show_result(Y_test, y_pred[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 260/260 [00:04<00:00, 59.66it/s]\n",
      "100%|██████████| 260/260 [00:04<00:00, 64.58it/s]\n",
      "100%|██████████| 260/260 [00:03<00:00, 68.54it/s]\n",
      "100%|██████████| 260/260 [00:03<00:00, 68.10it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.038461538461538464, 0.15, 0.23461538461538461, 0.40384615384615385)"
      ]
     },
     "execution_count": 285,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_hit_rate(gbdt,1),  test_hit_rate(gbdt,5), test_hit_rate(gbdt,10), test_hit_rate(gbdt,20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning",
   "language": "python",
   "name": "deeplearning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
